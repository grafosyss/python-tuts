{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Data Analysis with Python and Pandas Tutorial Introduction\n",
    " The Pandas module is a high performance, highly efficient, and high level data analysis library.At its core, it is very much like operating a headless version of a spreadsheet, like Excel. Most of the datasets you work with will be what are called datafra mes. You may be familiar with this term already, it is used across other languages, but, if not, a dataframe is most often just like a spreadsheet. Columns and rows, that's all there is to it! From here, we can utilize Pandas to perform operations on our data sets at lightning speeds.\n",
    " \n",
    " Here, we import pandas as pd. This is just a common standard used when importing the Pandas module. Next, we import datetime, which we'll use in a moment to tell Pandas some dates that we want to pull data between. Finally, we import pandas.io.data as web, because we're going to use this to pull data from the internet. \n",
    " \n",
    " Here, we create start and end variables that are datetime objects, pulling data from Jan 1st 2010 to today. Now, we can create a dataframe like so:\n",
    " \n",
    " This pulls data for Exxon from the Morningstar API (which we've had to change since the video, since both Yahoo and Google have stopped their APIs), storing the data to our df variable. Naming your dataframe df is not required, but again, is pretty popular standard for working with Pandas. It just helps people immediately identify the working dataframe without needing to trace the code back.\n",
    "\n",
    "So this gives us a dataframe, how do we see it? Well, can can just print it, like:\n",
    "\n",
    "So that's a lot of space. The middle of the dataset is ignored, but that's still a lot of output. Instead, most people will just do:\n",
    "\n",
    "Morningstar's api returns slightly more complex formatted results. We can clean this up to be just rows and columns like a spreadsheet might be with:\n",
    "\n",
    "This prints the first 5 rows of the dataframe, and is useful for debugging and just generally seeing what your dataframe looks like. As you perform analysis and such, this will be useful to see if what you intended actually happened or not. We'll dive more into this later on, however.\n",
    "\n",
    "We could stop here with the intro, but one more thing: Data Visualization. Like I said earlier, Pandas works great with other modules, Matplotlib being one of them. Let's see! Open your terminal or cmd.exe, and do pip install matplotlib. You should already have got it I am prety sure with your pandas installation, but we want to make sure. Now, at the top of your script with the other imports, add:\n",
    "\n",
    "Pyplot is the basic matplotlib graphing module. Style helps us quickly make our graphs look good, and style.use lets us choose a style. Interested in learning more about Matplotlib? Check out the in-depth Matplotlib tutorial series!\n",
    "\n",
    "Next, below our print(df.head()), we can do something like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Close   High     Low   Open    Volume\n",
      "Date                                             \n",
      "2010-01-01  68.19  68.86  68.110  68.84         0\n",
      "2010-01-04  69.15  69.26  68.190  68.72  27811317\n",
      "2010-01-05  69.42  69.45  68.800  69.19  30178588\n",
      "2010-01-06  70.02  70.60  69.340  69.45  35047453\n",
      "2010-01-07  69.80  70.06  69.415  69.90  27194133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a26550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "start = datetime.datetime(2010, 1, 1)\n",
    "end = datetime.datetime.now()\n",
    "\n",
    "df = web.DataReader(\"XOM\", \"morningstar\", start, end)\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "df = df.drop(\"Symbol\", axis=1)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "df['High'].plot()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Pandas Basics - p.2 Data Analysis with Python and Pandas Tutorial\n",
    "In this Data analysis with Python and Pandas tutorial, we're going to clear some of the Pandas basics. Data prior to being loaded into a Pandas Dataframe can take multiple forms, but generally it needs to be a dataset that can form to rows and columns. So maybe a dictionary like this:\n",
    "\n",
    "We can turn this dictionary to a dataframe by doing the following:\n",
    "\n",
    "Now what can we do? As seen before, you can call for a quick initial snippit by doing:\n",
    "\n",
    "You may also want the last few lines instead. For this, you can do something like:\n",
    "\n",
    "Finally, you can also put the number of the head or tail you want, like so:\n",
    "\n",
    "You can see here how there are these numbers on the left, 0,1,2,3,4,5 and so on, like line numbers. These numbers are actually your \"index.\" The index of a dataframe is what the data is related by, ordered by...etc. Generally, it is going to be the variable that connects all of the data. In this case, we never defined anything for this purpose, and it would be a challenge for Pandas to just somehow \"know\" what that variable was. Thus, when you do not define an index, Pandas will just make one for you like this. Looking at the data set right now, do you see a column that connects the others?\n",
    "\n",
    "The \"Day\" column fits that bill! Generally, if you have any dated data, the date will be the \"index\" as this is how all of the data points relate. There are many ways to identify the index, change the index, and so on. We'll cover a couple here. First, on any existing dataframe, we can set a new index like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day\n",
      "1    43\n",
      "2    34\n",
      "3    65\n",
      "4    56\n",
      "5    29\n",
      "6    76\n",
      "Name: Visitors, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "web_stats = {'Day':[1,2,3,4,5,6],\n",
    "             'Visitors':[43,34,65,56,29,76],\n",
    "             'Bounce Rate':[65,67,78,65,45,52]}\n",
    "\n",
    "df = pd.DataFrame(web_stats)\n",
    "\n",
    "df.set_index('Day', inplace=True)\n",
    "\n",
    "print(df.Visitors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 IO Basics - p.3 Data Analysis with Python and Pandas Tutorial\n",
    " we will begin discussing IO, or input/output, with Pandas, and begin with a realistic use-case. To get ample practice, a very useful website is Quandl. Quandl contains a plethora of free and paid data sources. What makes this location great is that the data is generally normalized, it's all in one place, and extracting the data is the same method. If you are using Python, and you access the Quandl data via their simple module, then the data is automatically returned to a dataframe. For the purposes of this tutorial, we're going to just manually download a CSV file instead, for learning purposes, since not every data source you find is going to have a nice and neat module for extracting the datasets.\n",
    " \n",
    " Let's say we're interested in maybe purchasing or selling a home in Austin, Texas. The zipcode there is 77006. We could go to the local housing listings and see what the current prices are, but this doesn't really give us any real historical information, so let's just try to get some data on this. Let's query for \"home value index 77006.\" Sure enough, we can see an index here. There's top, middle, lower tier, three bedroom, and so on. Let's say, sure, we got a a three bedroom house. Let's check that out. Turns out Quandl already provides graphs, but let's grab the dataset anyway, make our own graph, and maybe do some other analysis. Go to download, and choose CSV. Pandas is capable of IO with csv, excel data, hdf, sql, json, msgpack, html, gbq, stata, clipboard, and pickle data, and the list continues to grow. Check out the IO Tools documentation for the current list. Take that CSV and move it into the local directory (the directory that you are currently working in / where this .py script is).\n",
    "\n",
    "Starting with this code, loading in a CSV to a dataframe can be as simple as:\n",
    "\n",
    "Notice that we have no decent index again. We can fix that like we did before doing:\n",
    "\n",
    "Now, let's say we want to send this back to a CSV, we can do:\n",
    "\n",
    "We only have the one column right now, but if you had many columns, and just wanted to send one, you could do:\n",
    "\n",
    "Remember how we graphed multiple, but not all, columns? See if you can guess how to save multiple, but not all, columns.\n",
    "\n",
    "Now, let's read that new CSV in:\n",
    "\n",
    "Darn, our index is gone again! This is because CSV has no \"index\" attribute like our dataframe does. What we can do, is set the index on import, rather than importing and then setting the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('ZILL-Z77006_3B.csv')\n",
    "print(df.head())\n",
    "\n",
    "df.set_index('Date', inplace = True)\n",
    "\n",
    "df.to_csv('newcsv2.csv')\n",
    "\n",
    "df['Value'].to_csv('newcsv2.csv')\n",
    "\n",
    "df.columns = ['House_Prices']\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Building dataset - p.4 Data Analysis with Python and Pandas Tutorial\n",
    "In this part of Data Analysis with Python and Pandas tutorial series, we're going to expand things a bit. Let's consider that we're multi-billionaires, or multi-millionaires, but it's more fun to be billionaires, and we're trying to diversify our portfolio as much as possible. We want to have all types of asset classes, so we've got stocks, bonds, maybe a money market account, and now we're looking to get into real estate to be solid. You've all seen the commercials right? You buy a CD for $60, attend some $500 seminar, and you're set to start making your 6 figure at a time investments into property, right?\n",
    "\n",
    "Okay, maybe not, but we definitely want to do some research and have some sort of strategy for buying real estate. So, what governs the prices of homes, and do we need to do the research to find this out? Generally, no, you don't really need to do that digging, we know the factors. The factors for home prices are governed by: The economy, interest rates, and demographics. These are the three major influences in general for real estate value. Now, of course, if you're buying land, various other things matter, how level is it, are we going to need to do some work to the land before we can actually lay foundation, how is drainage etc. If there is a house, then we have even more factors, like the roof, windows, heating/AC, floors, foundation, and so on. We can begin to consider these factors later, but first we'll start at the macro level. You will see how quickly our data sets inflate here as it is, it'll blow up fast.\n",
    "\n",
    "So, our first step is to just collect the data. Quandl still represents a great place to start, but this time let's automate the data grabbing. We're going to pull housing data for the 50 states first, but then we stand to try to gather other data as well. We definitely dont want to be manually pulling this data. First, if you do not already have an account, you need to get one. This will give you an API key and unlimited API requests to the free data, which is awesome.\n",
    "\n",
    "Once you create an account, go to your account / me, whatever they are calling it at the time, and then find the section marked API key. That's your key, which you will need. Next, we want to grab the Quandl module. We really don't need the module to make requests at all, but it's a very small module, and the size is worth the slight ease it gives us, so might as well. Open up your terminal/cmd.exe and do pip install quandl (again, remember to specify the full path to pip if pip is not recognized).\n",
    "\n",
    "Next, we're ready to rumble, open up a new editor. To start:\n",
    "\n",
    "You can just store a plain-text version of your key if you want, I have only hidden mine since it's a tutorial that I am publishing. This is all we need to do to grab the housing price index for Texas. The actual ticker that we grab can be found on any page, whenever you get there, just click on the library you are using on the side, in our case, Python, and up will pop the query you need to type.\n",
    "\n",
    "As you carry on with your data science career, you will learn various constants that just happen to be because people are logical and reasonable. In our case, we need to grab the data for all of the states. How might we do this? Do we need to grab every ticker manually still? No. Looking at this ticker, we see FMAC/HPI_TX. We can easily decypher this to FMAC = Freddie Mac. HPI = House Price Index. TX means Texas, the common 2-letter abbreviation for the state. From here, we can safely assume all of the tickers are built this way, so now we just need a list of the state abbreviations. We Google it, make a choice like This list of the 50 states. Now what?\n",
    "\n",
    "We can extract this data in a variety of ways. This is a Pandas tutorial, so if we can use Pandas, we shall. Let's check out the read_html from Pandas. It's not being called \"experimental\" anymore, but I would still label this as expirimental. The standard and quality of the other IO modules is very high and dependable. This read_html is not quite up to par, but I still say it's very impressive and useful code, and just plain cool. The way it works is you just simply feed in a URL, and Pandas will extract the dataframe worthy date from tables into a dataframe. This means, unlike the other typical methods you will usually use, read_html ends up reading into a list of dataframes. This isn't the only one that is different, but it is different. First, in order to use read_html, we need html5lib. Open up cmd.exe or your terminal and do: pip install html5lib. Now, we can make our first attempt by doing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Quandl\n",
    "\n",
    "# Not necessary, I just do this so I do not show my API key.\n",
    "api_key = open('quandlapikey.txt','r').read()\n",
    "\n",
    "df = Quandl.get(\"FMAC/HPI_TX\", authtoken=api_key)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "fiddy_states = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states')\n",
    "print(fiddy_states)\n",
    "\n",
    "print(fiddy_states[0])\n",
    "\n",
    "for abbv in fiddy_states[0][0][1:]:\n",
    "    print(abbv)\n",
    "    \n",
    "for abbv in fiddy_states[0][0][1:]:\n",
    "    #print(abbv)\n",
    "    print(\"FMAC/HPI_\"+str(abbv))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Concatenating and Appending dataframes - p.5 Data Analysis with Python and Pandas Tutorial\n",
    "There are four major ways of combining dataframes, which we'll begin covering now. The four major ways are: Concatenation, joining, merging, and appending. We'll begin with Concatenation.\n",
    "Appending is like the first example of concatenation, only a bit more forceful in that the dataframe will simply be appended to, adding to rows.\n",
    "A series is basically a single-columned dataframe. A series does have an index, but, if you convert it to a list, it will be just those values.\n",
    "\n",
    "Welcome to Part 5 of our Data Analysis with Python and Pandas tutorial series. In this tutorial, we're going to be covering how to combine dataframes in a variety of ways.\n",
    "\n",
    "In our case with real estate investing, we're hoping to take the 50 dataframes with housing data and then just combine them all into one dataframe. We do this for multiple reasons. First, it is easier and just makes sense to combine these, but also it will result in less memory being used. Every dataframe has a date and value column. This date column is repeated across all the dataframes, but really they should all just share the one, effectively nearly halving our total column count.\n",
    "\n",
    "When combining dataframes, you might have quite a few goals in mind. For example, you may want to \"append\" to them, where you may be adding to the end, basically adding more rows. Or maybe you want to add more columns, like in our case. There are four major ways of combining dataframes, which we'll begin covering now. The four major ways are: Concatenation, joining, merging, and appending. We'll begin with Concatenation. Here are some starting dataframes:\n",
    "\n",
    "Notice there are two major changes between these. df1 and df3 have the same index, but they have some different columns. df2 and df3 have different indexes and some differing columns. With concatenation, we can talk about various methods of bringing these together. Let's try a simple concatenation:\n",
    "\n",
    "Easy enough. The major difference between these was merely a continuation of the index, but they shared the same columns. Now they have become a single dataframe. In our case, however, we're curious about adding columns, not rows. What happens when we combine some shared and some new:\n",
    "\n",
    "That's what we expect with an append. In most cases, you are going to do something like this, as if you're inserting a new row in a database. Dataframes were not really made to be appended efficiently, they are meant moreso to be manipulated based on their starting data, but you can append if you need to. What happens when we append data with the same index?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      HPI  Int_rate  US_GDP_Thousands\n",
      "2001   80         2                50\n",
      "2002   85         3                55\n",
      "2003   88         2                65\n",
      "2004   85         2                55\n",
      "2005   80         2                50\n",
      "2006   85         3                55\n",
      "2007   88         2                65\n",
      "2008   85         2                55\n",
      "      HPI  Int_rate  US_GDP_Thousands\n",
      "2001   80         2                50\n",
      "2002   85         3                55\n",
      "2003   88         2                65\n",
      "2004   85         2                55\n",
      "2005   80         2                50\n",
      "2006   85         3                55\n",
      "2007   88         2                65\n",
      "2008   85         2                55\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({'HPI':[80,85,88,85],\n",
    "                    'Int_rate':[2, 3, 2, 2],\n",
    "                    'US_GDP_Thousands':[50, 55, 65, 55]},\n",
    "                   index = [2001, 2002, 2003, 2004])\n",
    "\n",
    "df2 = pd.DataFrame({'HPI':[80,85,88,85],\n",
    "                    'Int_rate':[2, 3, 2, 2],\n",
    "                    'US_GDP_Thousands':[50, 55, 65, 55]},\n",
    "                   index = [2005, 2006, 2007, 2008])\n",
    "\n",
    "df3 = pd.DataFrame({'HPI':[80,85,88,85],\n",
    "                    'Int_rate':[2, 3, 2, 2],\n",
    "                    'Low_tier_HPI':[50, 52, 50, 53]},\n",
    "                   index = [2001, 2002, 2003, 2004])\n",
    "\n",
    "concat = pd.concat([df1,df2])\n",
    "print(concat)\n",
    "\n",
    "df4 = df1.append(df2)\n",
    "print(df4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 Joining and Merging Dataframes - p.6 Data Analysis with Python and Pandas Tutorial\n",
    " After mergin, you would probably set the new index. \n",
    " \n",
    " Welcome to Part 6 of the Data Analysis with Python and Pandas tutorial series. In this part, we're going to talk about joining and merging dataframes, as another method of combining dataframes. In the previous tutorial, we covered concatenation and appending.\n",
    "\n",
    "First we will start with some sample dataframes like before, with one change\n",
    "\n",
    "The only major change is in df3, where we change Int_rate to unemployment. First, let's discuss merging.\n",
    "\n",
    "Notice here there are two versions of the US_GDP_Thousands. This is because we didn't share on these columns, so both are retained, with another letter to differentiate. Remember before I was saying that Pandas is a great module to marry to a database like mysql? here's why.\n",
    "\n",
    "Generally, with databases, you want to keep them as lightweight as possible so the queries that run on them can execute as fast as possible.\n",
    "\n",
    "Let's say you run a website like pythonprogramming.net, where you have users, so you definitely want to track username and encrypted password hashes, so that's 2 columns for sure. Maybe then you have a login name, a username, a password, an email and a join date. So that's already 5 columns with basic data points. Then maybe you have something like user settings, posts if you have a forum, completed tutorials. Then maybe you want to have settings like admin, moderator, regular user.\n",
    "\n",
    "The lists can go on and on. If you have literally just 1 massive table, this can work, but it might also be better to distribute the table, since many operations will simply be much quicker and more efficient. After mergin, you would probably set the new index. Something like this:\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df4 = pd.merge(df1,df3, on='HPI')\n",
    "df4.set_index('HPI', inplace=True)\n",
    "print(df4)\n",
    "\n",
    "df1.set_index('HPI', inplace=True)\n",
    "df3.set_index('HPI', inplace=True)\n",
    "\n",
    "joined = df1.join(df3)\n",
    "print(joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 Pickling - p.7 Data Analysis with Python and Pandas Tutorial\n",
    "You generally train a classifier, and then you can start immediately, and quickly, classifying with that classifier. The problem is, a classifer can't be saved to a .txt or .csv file. It's an object. Luckily, in programming, there are various terms for the process of saving binary data to a file that can be accessed later. In Python, this is called pickling. You may know it as serialization, or maybe even something else. Python has a module called Pickle, which will convert your object to a byte stream, or the reverse with unpickling. What this lets us do is save any Python object. \n",
    "\n",
    "Welcome to Part 7 of our Data Analysis with Python and Pandas tutorial series. In the last couple tutorials, we learned how to combine data sets. In this tutorial, we're going to resume under the premise that we're aspiring real estate moguls. We're looking to protect our wealth by having diversified wealth, and, one component to this is real-estate. In Part 4, we had the following code set up:\n",
    "This code was used to get the 50 states, iterate through them, and generate the appropriate Quandl query to return the House Price Index by state. Since we'll wind up with 50 dataframes here, we'd rather combine them all into one massive one. To do this, we can use .join, which we learned in the previous tutorial. We will use .join in this case because the data is returned back to us, using the Quandl module, with an actual index, Date. Normally, you probably wont get this, it'll just be dataframes with regular numbers for the index. In that case, you would use concatenate, with on = 'Date.'\n",
    "\n",
    "Now, to run through and collect all of the data, we can do the following change:\n",
    "\n",
    "NOTE: Quandl has since changed the returns of datasets, to where if the return has one column (or so it seems to me), then the title of that column is just \"value.\" Well, that's irritating, but we can work around it. In our for loop, rename the dataframe's column to what our abbv value is. Without making this change, you will likely be seeing: ValueError: columns overlap but no suffix specified: Index([u'Value'], dtype='object')\n",
    "\n",
    "Great, but, you will find this process might take anywhere from 30 seconds to a few minutes, every single time you want to run it. That's pretty annoying. Right now, your short term goal is to just make this happen, but then what's next? We're going to keep building on this, and every time we run a test or whatever, we'll have to put up with this nonsense! Because of that, we want to save this data. Now, this is both a Data Analysis and a Pandas tutorial. With Pandas, we could just simply output the data to a CSV, or whatever data type we want, including what we're about to talk about. You may not always have the option to output the data to a simple file, however. In any case, we want to save this data to a file, so we just need to do this action once, then we can build on top of it.\n",
    "\n",
    "When it comes to something like, machine learning, for example. You generally train a classifier, and then you can start immediately, and quickly, classifying with that classifier. The problem is, a classifer can't be saved to a .txt or .csv file. It's an object. Luckily, in programming, there are various terms for the process of saving binary data to a file that can be accessed later. In Python, this is called pickling. You may know it as serialization, or maybe even something else. Python has a module called Pickle, which will convert your object to a byte stream, or the reverse with unpickling. What this lets us do is save any Python object. That machine learning classifier? Yep. Dictionary? Yessir. Dataframe? Yep! Now, it just so happens that Pandas has pickles handled in its IO module, but you really should know how to do it with and without Pandas, so let's do that!\n",
    "\n",
    "First, let's talk about a regular pickle. You can do this with any Python object that you want, it does not need to be a dataframe, but we'll do it with our dataframe.\n",
    "\n",
    "First, import pickle at the top of your script\n",
    "\n",
    "First we open a .pickle file with the intention to write some bytes. Then, we do pickle.dump to dump the data we want to pickle, and then where to dump it (the file we just opened). Finally, just any file, we close. Done, we've saved the pickle.\n",
    "\n",
    "I would like to have us organize this code now, however. We don't want to run this code every time, but we do still want to reference the state list likely from time to time. Let's clean this up a bit:\n",
    "\n",
    "Now, we can reference state_list any time we need that state list, and then we just call the grab_initial_state_data real quick for the HPI baseline, and we've saved that data to a pickle file.\n",
    "\n",
    "Now, to get that data again, we just need to do:\n",
    "\n",
    "The output is more than I want to paste here, but you should get a dataframe that is ~462 rows x 50 columns. There you have it. Part of the object is that it is a dataframe, it's our way of just \"saving\" a variable. Very cool! You can do this with the pickle module anywhere in Python, but it turns out that Pandas has its own pickle as well, so we might as well illustrate that:\n",
    "\n",
    "Again, output is a bit too large to paste here, but you should get the same thing. If you're like me, you might wonder \"why did Pandas make their own pickle option, if all of Python already has one that works just fine?\" I really do not know. Apparently, the Pandas one can sometimes be quicker on massive datasets.\n",
    "\n",
    "Now that we've got a pickle of our data, we're ready to continue along in the next tutorial with further research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Quandl\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Not necessary, I just do this so I do not show my API key.\n",
    "api_key = open('quandlapikey.txt','r').read()\n",
    "\n",
    "def state_list():\n",
    "    fiddy_states = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states')\n",
    "    return fiddy_states[0][0][1:]\n",
    "    \n",
    "\n",
    "def grab_initial_state_data():\n",
    "    states = state_list()\n",
    "\n",
    "    main_df = pd.DataFrame()\n",
    "\n",
    "    for abbv in states:\n",
    "        query = \"FMAC/HPI_\"+str(abbv)\n",
    "        df = Quandl.get(query, authtoken=api_key)\n",
    "        print(query)\n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df)\n",
    "            \n",
    "    pickle_out = open('fiddy_states.pickle','wb')\n",
    "    pickle.dump(main_df, pickle_out)\n",
    "    pickle_out.close()        \n",
    "\n",
    "    \n",
    "grab_initial_state_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 Percent Change and Correlation Tables - p.8 Data Analysis with Python and Pandas Tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grab_initial_state_data():\n",
    "    states = state_list()\n",
    "\n",
    "    main_df = pd.DataFrame()\n",
    "\n",
    "    for abbv in states:\n",
    "        query = \"FMAC/HPI_\"+str(abbv)\n",
    "        df = Quandl.get(query, authtoken=api_key)\n",
    "        print(query)\n",
    "        df[abbv] = (df[abbv]-df[abbv][0]) / df[abbv][0] * 100.0\n",
    "        print(df.head())\n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df)\n",
    "            \n",
    "    pickle_out = open('fiddy_states3.pickle','wb')\n",
    "    pickle.dump(main_df, pickle_out)\n",
    "    pickle_out.close()\n",
    "\t\n",
    "grab_initial_state_data()   \n",
    "\n",
    "HPI_data = pd.read_pickle('fiddy_states3.pickle')\n",
    "\n",
    "HPI_data.plot()\n",
    "plt.legend().remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 Resampling - p.9 Data Analysis with Python and Pandas Tutorial\n",
    "smoothing out data by removing noise. There are two main methods to do this. The most popular method used is what is called resampling, \n",
    "\n",
    "Welcome to another data analysis with Python and Pandas tutorial. In this tutorial, we're going to be talking about smoothing out data by removing noise. There are two main methods to do this. The most popular method used is what is called resampling, though it might take many other names. This is where we have some data that is sampled at a certain rate. For us, we have the Housing Price Index sampled at a one-month rate, but we could sample the HPI every week, every day, every minute, or more, but we could also resample at every year, every 10 years, and so on.\n",
    "\n",
    "Another environment where resampling almost always occurs is with stock prices, for example. Stock prices are intra-second. What winds up happening though, is usually stock prices are resampled to minute data at the lowest for free data. You can buy access to live data, however. On a long-term scale, usually the data will be sampled daily, or even every 3-5 days. This is often done to keep the size of the data being transferred low. For example, over the course of, say, one year, intra-second data is usually in the multiples of gigabytes, and transferring all of that at once is unreasonable and people would be waiting minutes or hours for pages to load.\n",
    "\n",
    "Using our current data, which is currently sampled at once a month, how might we sample it instead to once every 6 months, or 2 years? Try to think about how you might personally write a function that might perform that task, it's a fairly challenging one, but it can be done. That said, it's a fairly computationally inefficient job, but Pandas has our backs and does it very fast. Let's see. Our starting script right now:\n",
    "\n",
    "First, let's make this a bit more basic, and just reference the Texas information first, but also resample it:\n",
    "\n",
    "We resampled with the \"A,\" which resamples annually (year-end). You can find all of the resample options here: http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases, but here are the current ones as of my writing this tutorial:\n",
    "As you can see, moving from monthly to annual data has not really hidden anything from us regarding the trend line itself, but one interesting thing to note, however, at least here for Texas, do you think those wiggles in the monthly data look a bit patterned? I do. You can hover your mouse over all the peaks, and start looking at the month out of the year that these peaks are occuring. Most of the peaks are occuring around 6 months in, with just about every low occuring around December. A lot of states have this pattern, and it comes through in the US HPI. Maybe we will just play those trends and be done with the entire tutorial! We are now experts!\n",
    "\n",
    "Okay not really, I guess we'll continue on with the tutorial. So with resampling, we can choose the interval, as well as \"how\" we wish to resample. The default is by mean, but there's also a sum of that period. If we resampled by year, with how=sum, then the return would be a sum of all the HPI values in that 1 year. Finally, there's OHLC, which is open high low and close. This returns the starting value, the highest value, the lowest value, and the last value in that period.\n",
    "\n",
    "I think we're better off sticking with the monthly data, but resampling is definitely worth covering in any Pandas tutorial. Now, you may be wondering why we made a new dataframe for the resampling rather than just adding it to our existing dataframe. The reason for this is it would have created a large amount of NaN data. Sometimes, even just the original resampling will contain NaN data, especially if your data is not updated by uniform intervals. Handling missing data is a major topic, but we'll attempt to cover it broadly in the next tutorial, both with the philosophy of handling missing data as well as how to handle your choices via your program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Quandl\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "# Not necessary, I just do this so I do not show my API key.\n",
    "api_key = open('quandlapikey.txt','r').read()\n",
    "\n",
    "def state_list():\n",
    "    fiddy_states = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states')\n",
    "    return fiddy_states[0][0][1:]\n",
    "    \n",
    "\n",
    "def grab_initial_state_data():\n",
    "    states = state_list()\n",
    "\n",
    "    main_df = pd.DataFrame()\n",
    "\n",
    "    for abbv in states:\n",
    "        query = \"FMAC/HPI_\"+str(abbv)\n",
    "        df = Quandl.get(query, authtoken=api_key)\n",
    "        print(query)\n",
    "        df[abbv] = (df[abbv]-df[abbv][0]) / df[abbv][0] * 100.0\n",
    "        print(df.head())\n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df)\n",
    "            \n",
    "    pickle_out = open('fiddy_states3.pickle','wb')\n",
    "    pickle.dump(main_df, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "def HPI_Benchmark():\n",
    "    df = Quandl.get(\"FMAC/HPI_USA\", authtoken=api_key)\n",
    "    df[\"United States\"] = (df[\"United States\"]-df[\"United States\"][0]) / df[\"United States\"][0] * 100.0\n",
    "    return df\n",
    "fig = plt.figure()\n",
    "ax1 = plt.subplot2grid((1,1), (0,0))\n",
    "HPI_data = pd.read_pickle('fiddy_states3.pickle')\n",
    "HPI_State_Correlation = HPI_data.corr()\n",
    "\n",
    "TX1yr = HPI_data['TX'].resample('A')\n",
    "print(TX1yr.head())\n",
    "\n",
    "HPI_data['TX'].plot(ax=ax1)\n",
    "TX1yr.plot(color='k',ax=ax1)\n",
    "\n",
    "plt.legend().remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 Handling Missing Data - p.10 Data Analysis with Python and Pandas Tutorial\n",
    "We have a few options when considering the existence of missing data.\n",
    "\n",
    "Ignore it - Just leave it there\n",
    "Delete it - Remove all cases. Remove from data entirely. This means forfeiting the entire row of data.\n",
    "Fill forward or backwards - This means taking the prior or following value and just filling it in.\n",
    "Replace it with something static - For example, replacing all NaN data with -9999.\n",
    "\n",
    "Welcome to Part 10 of our Data Analysis with Python and Pandas tutorial. In this part, we're going to be talking about missing or not available data. We have a few options when considering the existence of missing data.\n",
    "\n",
    "Ignore it - Just leave it there\n",
    "Delete it - Remove all cases. Remove from data entirely. This means forfeiting the entire row of data.\n",
    "Fill forward or backwards - This means taking the prior or following value and just filling it in.\n",
    "Replace it with something static - For example, replacing all NaN data with -9999.\n",
    "Each of these options has their own merits for a variety of reasons. Ignoring it requires no more work on our end. You may choose to ignore missing data for legal reasons, or maybe to retain the utmost integrity of the data. Missing data might also be very important data. For example, maybe part of your analysis is investigating signal drops from a server. In this case, maybe the missing data is super important to keep in the set.\n",
    "\n",
    "Next, we have delete it. You have another two choices at this point. You can either delete rows if they contain any amount of NaN data, or you can delete the row if it is completely NaN data. Usually a row that is full of NaN data comes from a calculation you performed on the dataset, and no data is really missing, it's just simply not available given your formula. In most cases, you would at least want to drop all rows that are completely NaN, and in many cases you would like to just drop rows that have any NaN data. How might we go about doing this? We'll start with the following script (notice the resampling is being done now by adding a new column to the HPI_data dataframe.):\n",
    "\n",
    "We have lots of NaN data. If we uncomment all the graphing code, what happens? Turns out, we don't get the graph that contains NaN data! This is a bummer, so first we're thinking, okay, let's drop all the rows that have any NaN data. This is just for tutorial purposes. In this example, that'd be a pretty bad idea to do. Instead, you would want to do what we originally did, which was create a new dataframe for the resampled data. This doesn't mean that's what you'd always do, but, in this case, you would. Anyway, let's drop all rows that contain any na data. That's as simple as:\n",
    "\n",
    "Okay, great. Now just for tutorial purposes, how might we write code that only deletes rows if the full row is NaN?\n",
    "\n",
    "For the \"how\" parameter, you can choose between all or any. All requires all data in the row to be NaN for you to delete it. You can also choose \"any\" and then set a threshold. This threshold will require that many non-NA values to accept the row. See the Pandas documentation for dropna for more information.\n",
    "\n",
    "Alright, so that's dropna, next we have filling it. With filling, we have two major options again, which is whether to fill forward, backwards. The other option is to just replace the data, but we called that a separate choice. It just so happens that the same function is used to do it, fillna().\n",
    "\n",
    "Modifying our original code block, with the main change of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HPI_data.dropna(inplace=True)\n",
    "print(HPI_data[['TX','TX1yr']])\n",
    "\n",
    "HPI_data.dropna(how='all',inplace=True)\n",
    "\n",
    "HPI_data.fillna(method='ffill',inplace=True)\n",
    "\n",
    "HPI_data.fillna(method='bfill',inplace=True)\n",
    "\n",
    "HPI_data.fillna(value=-99999,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11 Rolling statistics - p.11 Data Analysis with Python and Pandas Tutorial\n",
    "One of the more popular rolling statistics is the moving average. This takes a moving window of time, and calculates the average or the mean of that time period as the current value. In our case, we have monthly data. So a 10 moving average would be the current value, plus the previous 9 months of data, averaged, and there we would have a 10 moving average of our monthly data. Doing this is Pandas is incredibly fast. Pandas comes with a few pre-made rolling statistical functions, but also has one called a rolling_apply. This allows us to write our own function that accepts window data and apply any bit of logic we want that is reasonable. This means that even if Pandas doesn't officially have a function to handle what you want, they have you covered and allow you to write exactly what you need. Let's start with a basic moving average, or a rolling_mean as Pandas calls it. \n",
    "\n",
    "Welcome to another data analysis with Python and Pandas tutorial series, where we become real estate moguls. In this tutorial, we're going to be covering the application of various rolling statistics to our data in our dataframes.\n",
    "\n",
    "One of the more popular rolling statistics is the moving average. This takes a moving window of time, and calculates the average or the mean of that time period as the current value. In our case, we have monthly data. So a 10 moving average would be the current value, plus the previous 9 months of data, averaged, and there we would have a 10 moving average of our monthly data. Doing this is Pandas is incredibly fast. Pandas comes with a few pre-made rolling statistical functions, but also has one called a rolling_apply. This allows us to write our own function that accepts window data and apply any bit of logic we want that is reasonable. This means that even if Pandas doesn't officially have a function to handle what you want, they have you covered and allow you to write exactly what you need. Let's start with a basic moving average, or a rolling_mean as Pandas calls it. You can check out all of the Moving/Rolling statistics from Pandas' documentation.\n",
    "\n",
    "This gives us a new column, which we've named TX12MA to reflect Texas, and 12 moving average. We apply this with pd.rolling_mean(), which takes 2 main parameters, the data we're applying this to, and the periods/windows that we're doing.\n",
    "\n",
    "With rolling statistics, NaN data will be generated initially. Consider doing a 10 moving average. On row #3, we simply do not have 10 prior data points. Thus, NaN data will form. You can either just leave it there, or remove it with a dropna(), covered in the previous tutorial.\n",
    "\n",
    "Another interesting one is rolling standard deviation. We'd need to put that on its own graph, but we can do that:\n",
    "\n",
    "Here, we defined a 2nd axis, as well as changing our size. We said this grid for subplots is a 2 x 1 (2 tall, 1 wide), then we said ax1 starts at 0,0 and ax2 starts at 1,0, and it shares the x axis with ax1. This allows us to zoom in on one graph and the other zooms in to the same point. Confused still about Matplotlib? Check out the full Data Visualization with Matplotlib tutorial series.\n",
    "\n",
    "Then we graphed everything.\n",
    "\n",
    "Another interesting visualization would be to compare the Texas HPI to the overall HPI. Then do a rolling correlation between the two of them. The assumption would be that when correlation was falling, there would soon be a reversion. If correlation was falling, that'd mean the Texas HPI and the overall HPI were diverging. Let's say the overall US HPI was on top and TX_HPI was diverging below. In this case, we may choose to invest in TX real-estate. Another option would be to use TX and another area that has high correlation with it. Texas, for example had a 0.983235 correlation with Alaska. Let's see how our plan would look visually. The ending block should now look like:\n",
    "\n",
    "Every time correlation drops, you should in theory sell property in the are that is rising, and then you should buy property in the area that is falling. The idea is that, these two areas are so highly correlated that we can be very confident that the correlation will eventually return back to about 0.98. As such, when correlation is -0.5, we can be very confident in our decision to make this move, as the outcome can be one of the following: HPI forever diverges like this and never returns (unlikely), the falling area rises up to meet the rising one, in which case we win, the rising area falls to meet the other falling one, in which case we made a great sale, or both move to re-converge, in which case we definitely won out. It's unlikely with HPI that these markets will fully diverge permanantly. We can see clearly that this just simply doesnt happen, and we've got 40 years of data to back that up.\n",
    "\n",
    "In the next tutorial, we're going to talk about detecting outliers, both erroneous and not, and include some of the philsophy behind how to handle such data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HPI_data['TX12MA'] = pd.rolling_mean(HPI_data['TX'], 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 Applying Comparison Operators to DataFrame - p.12 Data Analysis with Python and Pandas Tutorial\n",
    "we're goign to talk briefly on the handling of erroneous/outlier data. Just because data is an outlier, it does not mean it is erroneous. A lot of times, an outlier data point can nullify a hypothesis, so the urge to just get rid of it can be high, but this isn't what we're talking about here.\n",
    "\n",
    "What would an erroneous outlier be? An example I like to use is when measuring fluctuations in something like, say, a bridge. As bridges carry weight, they can move a bit. In storms, that can wiggle about a bit, there is some natural movement. As time goes on, and supports weaken, the bridge might move a bit too much, and eventually need to be reinforced. Maybe we have a system in place that constantly measures fluctuations in the bridge's height.\n",
    "\n",
    "Preprocessing is used to adjust our dataset. Typically, machine learning will be a bit more accurate if your features are between -1 and 1 values.\n",
    "\n",
    "Welcome to part 12 of the Data Analysis with Python and Pandas tutorial series. In this tutorial, we're goign to talk briefly on the handling of erroneous/outlier data. Just because data is an outlier, it does not mean it is erroneous. A lot of times, an outlier data point can nullify a hypothesis, so the urge to just get rid of it can be high, but this isn't what we're talking about here.\n",
    "\n",
    "What would an erroneous outlier be? An example I like to use is when measuring fluctuations in something like, say, a bridge. As bridges carry weight, they can move a bit. In storms, that can wiggle about a bit, there is some natural movement. As time goes on, and supports weaken, the bridge might move a bit too much, and eventually need to be reinforced. Maybe we have a system in place that constantly measures fluctuations in the bridge's height.\n",
    "\n",
    "Some distance sensors use lasers, others bounce sound waves. Whichever you want to pretend we're using, it doesn't matter. We'll pretend sound waves. The way these work is they emit sound waves from the trigger, which then bounce of the object in front, coming back to the receiver. From here, amount of time for this entire operation to occur is accounted for. Since the speed of sound is a constant, we can extrapolate from the time this process took, the distance the sound waves traveled. The problem is, this is only a measure of how far the sound waves traveled. There is no 100% certainty that they went to the bridge and back, for example. Maybe a leaf fell just as the measurement was being taken and bounced the signal around a bunch before it got back to the receiver, who knows. Let's say, for example though, you had the following readings for the bridge:\n",
    "\n",
    "So, did the bridge just get abducted by aliens? Since we have more normal readings after this, it's more likely that 6212.42 was just a bad reading though. We can tell visually that it is an outlier, but how could we detect this via our program?\n",
    "\n",
    "We realize this is an outlier because it differs so greatly from the other values, as well as the fact that it suddenly jumps and drops way more than any of the others. Soundes like we're just applying standard deviation here. Let's use that to automatically detect this bad reading.\n",
    "\n",
    "First, we get all the description. Mostly just showing that so you see how we're working with the data. Then, we get get straight to the meters' std, which is 2067 and some change. That's a pretty high figure, but it's still much lower than the STD for that major fluctuation (4385). Now, we can run through and remove all data that has a standard deviation higher than that.\n",
    "\n",
    "This allows us to learn a new skill: Logically modifying the dataframe! What we can do is something like:\n",
    "\n",
    "The new line we just learned was this df = df[ (df['STD'] < df_std) ]. How did that work? First, we start by re-defining df. We're saying df is equal now to df, where df['STD'] is less than the overall df_std that we calculated before. Thus, the only remaining data here will be data where the standard deviation is less than that 2067.\n",
    "\n",
    "Again, we're allowed to just remove this data when we know it's erroneous. Deleting data because it just doesn't \"suit\" you is almost always a bad idea.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bridge_height = {'meters':[10.26, 10.31, 10.27, 10.22, 10.23, 6212.42, 10.28, 10.25, 10.31]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13 Scikit Learn Incorporation\n",
    "In this Data Analysis with Pandas and Python tutorial series, we're going to show how quickly we can take our Pandas dataset in the dataframe and convert it to, for example, a numpy array, which can then be fed through a variety of other data analysis Python modules. The example that we're going to use here is Scikit-Learn, or SKlearn. In order to do this, you will need to install it:\n",
    "\n",
    "From here, we're almost already done. For machine learning to take place, at least in the supervised form, we need only a couple things. First, we need \"features.\" In our case, features are things like current HPI, maybe the GDP, and so on. Then you have \"labels.\" Labels are assigned to the feature \"sets,\" where a feature set is the collective GDP, HPI, and so on for any given \"label.\" Our label, in this case, is either a 1 or a 0, where 1 means the HPI increased in the future, and a 0 means it did not.\n",
    "\n",
    "It should probably go without saying, but I will note: You should not include the \"future HPI\" column as a feature. If you do this, the machine learning algorithm will recognize this and have a very high accuracy that would be impossible to actually use in the real world.\n",
    "\n",
    "We're going to use the svm (support vector machine) library for our machine learning classifier. Preprocessing is used to adjust our dataset. Typically, machine learning will be a bit more accurate if your features are between -1 and 1 values. This does not mean this will always be true, always a good idea to check with and without the scaling that we'll do to be safe. Cross_validation is a library that we'll be using to create our training and testing sets. It's just a nice way to automatically, and randomly, sample out your data for training and testing purposes.\n",
    "\n",
    "Now, we can create our features and our labels for training/testing:\n",
    "\n",
    "Generally, with features and labels, you have X, y. The uppercase X is used to denote a feature set. The y is the label. What we've done here, is define the featureset as the numpy array (this just converts the dataframe's contents to a multi-dimensional array) of the housing_data dataframe's contents, with the \"label\" and the \"US_HPI_future\" columns removed.\n",
    "\n",
    "Now our labels are defined, and we're ready to split up our data into training and testing sets. We can do this ourselves, but we'll use the cross_validation import from earlier:\n",
    "\n",
    "What this does is it splits up your features (X) and labels (y) into random training and testing groups for you. As you can see, the return is the feature training set, feature testing set, labels training, and labels testing. We then are unpacking these into X_train, X_test, y_train, y_test. cross_validation.train_test_split takes the parameters of your features, labels, and then you can also specify the testing size (test_size), which we've designated to be 0.2, meaning 20%.\n",
    "\n",
    "Now, we can establish the classifier that we intend to use:\n",
    "\n",
    "We're going to use support vector classifcation with a linear kernel in this example. Learn more about sklearn.svm.SVC here.\n",
    "\n",
    "Next, we want to train our classifier:\n",
    "Finally, we could actually go ahead and make predictions from here, but let's test the classifier's accuracy on known data:\n",
    "\n",
    "I am getting an average of about 70%ish accuracy. You may get differing results. There are many areas for adjusting machine learning. We could change some of the default parameters, we could check out some of the other algorithms, but this is decent enough for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Quandl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "# Not necessary, I just do this so I do not show my API key.\n",
    "api_key = open('quandlapikey.txt','r').read()\n",
    "\n",
    "def create_labels(cur_hpi, fut_hpi):\n",
    "    if fut_hpi > cur_hpi:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def moving_average(values):\n",
    "    return mean(values)\n",
    "\n",
    "housing_data = pd.read_pickle('HPI.pickle')\n",
    "housing_data = housing_data.pct_change()\n",
    "housing_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "housing_data['US_HPI_future'] = housing_data['United States'].shift(-1)\n",
    "housing_data.dropna(inplace=True)\n",
    "#print(housing_data[['US_HPI_future','United States']].head())\n",
    "housing_data['label'] = list(map(create_labels,housing_data['United States'], housing_data['US_HPI_future']))\n",
    "#print(housing_data.head())\n",
    "housing_data['ma_apply_example'] = pd.rolling_apply(housing_data['M30'], 10, moving_average)\n",
    "print(housing_data.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, preprocessing, cross_validation\n",
    "\n",
    "X = np.array(housing_data.drop(['label','US_HPI_future'], 1))\n",
    "X = preprocessing.scale(X)\n",
    "y = np.array(housing_data['label'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_test, y_test))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
